{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahavird/Resnet_Imagenent_1k/blob/main/imagenet_poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06b3586-c8bb-4cb9-8772-944b7c8954d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f06b3586-c8bb-4cb9-8772-944b7c8954d7",
        "outputId": "d30a9f3c-4f1e-487b-cd09-e7a1ed0b10c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported - ready to use PyTorch 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Import PyTorch libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)\n",
        "\n",
        "def show_image(image, label):\n",
        "    image = image.permute(1, 2, 0)\n",
        "    plt.imshow(image.squeeze())\n",
        "    plt.title(f'Label: {label}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0ded67-615c-4efd-ab7d-73217abc62c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a0ded67-615c-4efd-ab7d-73217abc62c0",
        "outputId": "3def29e0-6d37-4a5e-defe-759ba79c1708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'batch_size': 16, 'name': 'resnet_152_sgd1', 'workers': 4, 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001, 'lr_step_size': 30, 'lr_gamma': 0.1},\n",
              " 16)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# device\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "# device=\"cpu\"\n",
        "\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# resume training options\n",
        "resume_training = True\n",
        "\n",
        "class Params:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 16\n",
        "        self.name = \"resnet_152_sgd1\"\n",
        "        self.workers = 4\n",
        "        self.lr = 0.1\n",
        "        self.momentum = 0.9\n",
        "        self.weight_decay = 1e-4\n",
        "        self.lr_step_size = 30\n",
        "        self.lr_gamma = 0.1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.__dict__)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.__dict__ == other.__dict__\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "params = Params()\n",
        "params, params.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4zjhG5lV0e8N",
      "metadata": {
        "id": "4zjhG5lV0e8N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Onmk4rEPr_00",
      "metadata": {
        "id": "Onmk4rEPr_00"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ihj3_rO700X4",
      "metadata": {
        "id": "Ihj3_rO700X4"
      },
      "outputs": [],
      "source": [
        "%mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4xzgUFHm0ft9",
      "metadata": {
        "id": "4xzgUFHm0ft9"
      },
      "outputs": [],
      "source": [
        "!cp drive/MyDrive/data/imagenet/imagenet_subset.zip data/\n",
        "!cp drive/MyDrive/data/imagenet/imagenet_val.zip data/\n",
        "!unzip /content/data/archive.zip -d /content/data/\n",
        "!unzip /content/data/imagenet_val.zip -d /content/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rEh6-fttzQaa",
      "metadata": {
        "id": "rEh6-fttzQaa"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z1UZz06R804Q",
      "metadata": {
        "id": "z1UZz06R804Q"
      },
      "outputs": [],
      "source": [
        "!unzip /content/data/imagenet_subset.zip -d /content/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-0AnSnte8mYf",
      "metadata": {
        "id": "-0AnSnte8mYf"
      },
      "outputs": [],
      "source": [
        "! ls /content/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DB6NoBv3zT-t",
      "metadata": {
        "id": "DB6NoBv3zT-t"
      },
      "outputs": [],
      "source": [
        "!ls /content/data/imagenet_subtrain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ba5ba847-2b7a-478b-b3bf-73e2988e4595",
      "metadata": {
        "id": "ba5ba847-2b7a-478b-b3bf-73e2988e4595"
      },
      "outputs": [],
      "source": [
        "training_folder_name = '/content/drive/MyDrive/era4-assign9/sample-data-train'\n",
        "val_folder_name = '/content/drive/MyDrive/era4-assign9/sample-data-val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D-h3MxGd1utp",
      "metadata": {
        "id": "D-h3MxGd1utp"
      },
      "outputs": [],
      "source": [
        "os.listdir(training_folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4be1f5f-9eed-4afd-9dd3-1469e806d72c",
      "metadata": {
        "id": "b4be1f5f-9eed-4afd-9dd3-1469e806d72c"
      },
      "outputs": [],
      "source": [
        "os.listdir(training_folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "990b4f03-ff29-4ab4-b79b-c96dfccc0551",
      "metadata": {
        "id": "990b4f03-ff29-4ab4-b79b-c96dfccc0551"
      },
      "outputs": [],
      "source": [
        "train_transformation = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomResizedCrop(224, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        # Normalize the pixel values (in R, G, and B channels)\n",
        "        transforms.Normalize(mean=[0.485, 0.485, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "    root=training_folder_name,\n",
        "    transform=train_transformation\n",
        ")\n",
        "train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=params.batch_size,\n",
        "    sampler=train_sampler,\n",
        "    num_workers = params.workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "for X, y in train_loader:\n",
        "    break\n",
        "print(X.shape)\n",
        "show_image(X[0], y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "881862d6-1704-4300-b009-c13a0434d685",
      "metadata": {
        "id": "881862d6-1704-4300-b009-c13a0434d685"
      },
      "outputs": [],
      "source": [
        "train_dataset[1337]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a850ffa1-96ce-4a13-9731-b9ec39fddaed",
      "metadata": {
        "id": "a850ffa1-96ce-4a13-9731-b9ec39fddaed"
      },
      "outputs": [],
      "source": [
        "val_transformation = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(size=256, antialias=True),\n",
        "        transforms.CenterCrop(224),\n",
        "        # Normalize the pixel values (in R, G, and B channels)\n",
        "        transforms.Normalize(mean=[0.485, 0.485, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "val_dataset = torchvision.datasets.ImageFolder(\n",
        "    root=val_folder_name,\n",
        "    transform=val_transformation\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=64,\n",
        "    num_workers=params.workers,\n",
        "    shuffle=False,\n",
        "    pin_memory=True\n",
        ")\n",
        "for X, y in val_loader:\n",
        "    break\n",
        "print(X.shape)\n",
        "show_image(X[0], y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yolgYe7cFSM-",
      "metadata": {
        "id": "yolgYe7cFSM-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "def dataset_eda(train_dataset, val_dataset):\n",
        "    print(\"ðŸ“Š EDA on ImageNet Subset\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Total Training Images: {len(train_dataset)}\")\n",
        "    print(f\"Total Validation Images: {len(val_dataset)}\")\n",
        "    print(f\"Number of Classes: {len(train_dataset.classes)}\\n\")\n",
        "\n",
        "    # Print first few class names\n",
        "    print(\"Sample Class Names:\", train_dataset.classes[:10], \"\\n\")\n",
        "\n",
        "    # Count images per class\n",
        "    train_counts = Counter([train_dataset.targets[i] for i in range(len(train_dataset))])\n",
        "    val_counts = Counter([val_dataset.targets[i] for i in range(len(val_dataset))])\n",
        "\n",
        "    # Convert to sorted lists for visualization\n",
        "    class_indices = list(range(len(train_dataset.classes)))\n",
        "    train_freqs = [train_counts[i] for i in class_indices]\n",
        "    val_freqs = [val_counts[i] for i in class_indices]\n",
        "\n",
        "    # Plot class distribution (Top 30 only for readability)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x=list(range(min(30, len(train_freqs)))), y=train_freqs[:30])\n",
        "    plt.title(\"Training Samples per Class (Top 30)\")\n",
        "    plt.xlabel(\"Class Index\")\n",
        "    plt.ylabel(\"Image Count\")\n",
        "    plt.show()\n",
        "\n",
        "    # Show a few sample images\n",
        "    print(\"\\nðŸ–¼ï¸ Sample Images from Training Set:\")\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        idx = np.random.randint(len(train_dataset))\n",
        "        img, label = train_dataset[idx]\n",
        "        img = img.permute(1, 2, 0).numpy()\n",
        "        img = np.clip((img * 0.229 + 0.485), 0, 1)  # De-normalize for viewing\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(train_dataset.classes[label][:15])\n",
        "        ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run EDA\n",
        "dataset_eda(train_dataset, val_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e690d8-ee01-460f-a564-dd8d2c25d3b4",
      "metadata": {
        "id": "d1e690d8-ee01-460f-a564-dd8d2c25d3b4"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "def train(dataloader, model, loss_fn, optimizer, epoch, writer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    start0 = time.time()\n",
        "    start = time.time()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_size = len(X)\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * batch_size\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}], {(current/size * 100):>4f}%\")\n",
        "            step = epoch * size + current\n",
        "            writer.add_scalar('training loss',\n",
        "                            loss,\n",
        "                            step)\n",
        "            new_start = time.time()\n",
        "            delta = new_start - start\n",
        "            start = new_start\n",
        "            if batch != 0:\n",
        "                print(\"Done in \", delta, \" seconds\")\n",
        "                remaining_steps = size - current\n",
        "                speed = 100 * batch_size / delta\n",
        "                remaining_time = remaining_steps / speed\n",
        "                print(\"Remaining time (seconds): \", remaining_time)\n",
        "        optimizer.zero_grad()\n",
        "    print(\"Entire epoch done in \", time.time() - start0, \" seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf59e3d-06c3-4c89-8bb6-3cbd49ebd5b7",
      "metadata": {
        "id": "0cf59e3d-06c3-4c89-8bb6-3cbd49ebd5b7"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn, epoch, writer, train_dataloader, calc_acc5=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct, correct_top5 = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            if calc_acc5:\n",
        "                _, pred_top5 = pred.topk(5, 1, largest=True, sorted=True)\n",
        "                correct_top5 += pred_top5.eq(y.view(-1, 1).expand_as(pred_top5)).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    step = epoch * len(train_dataloader.dataset)\n",
        "    if writer != None:\n",
        "        writer.add_scalar('test loss',\n",
        "                            test_loss,\n",
        "                            step)\n",
        "    correct /= size\n",
        "    correct_top5 /= size\n",
        "    if writer != None:\n",
        "        writer.add_scalar('test accuracy',\n",
        "                            100*correct,\n",
        "                            step)\n",
        "        if calc_acc5:\n",
        "            writer.add_scalar('test accuracy5',\n",
        "                            100*correct_top5,\n",
        "                            step)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    if calc_acc5:\n",
        "        print(f\"Test Error: \\n Accuracy-5: {(100*correct_top5):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a691742a-c777-43e2-af73-58e9495adf61",
      "metadata": {
        "id": "a691742a-c777-43e2-af73-58e9495adf61"
      },
      "outputs": [],
      "source": [
        "## testing a pretrained model to validate correctness of our dataset, transform and metrics code\n",
        "pretrained_model = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT').to(device)\n",
        "start = time.time()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "test(val_loader, pretrained_model, loss_fn, epoch=0, writer=None, train_dataloader=train_loader, calc_acc5=True)\n",
        "print(\"Elapsed: \", time.time() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c912fcc-2ae7-41f0-8a7b-835e8bb8b763",
      "metadata": {
        "id": "8c912fcc-2ae7-41f0-8a7b-835e8bb8b763"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=1,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506b2306-a7e3-481d-aa4a-e7d83e2889c1",
      "metadata": {
        "id": "506b2306-a7e3-481d-aa4a-e7d83e2889c1"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Any, Callable, List, Optional, Type, Union\n",
        "from torch import Tensor\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        base_width: int = 64,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0))\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e92946-8b3b-4f5a-8169-a82d5f3cd73e",
      "metadata": {
        "id": "59e92946-8b3b-4f5a-8169-a82d5f3cd73e"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Bottleneck],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        width_per_group: int = 64,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(\n",
        "        self,\n",
        "        block: Type[Union[Bottleneck]],\n",
        "        planes: int,\n",
        "        blocks: int,\n",
        "        stride: int = 1,\n",
        "    ) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.base_width, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    base_width=self.base_width,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f89b61-29ad-4c61-a02a-55df6c493b22",
      "metadata": {
        "id": "29f89b61-29ad-4c61-a02a-55df6c493b22"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "model = ResNet(Bottleneck, [3, 8, 36, 3]).to(device)\n",
        "preds = model(X.to(device))\n",
        "preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cee08a9-32af-4f99-b096-e1ecf3f4a4f3",
      "metadata": {
        "id": "5cee08a9-32af-4f99-b096-e1ecf3f4a4f3"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),\n",
        "                            lr=params.lr, momentum=params.momentum, weight_decay=params.weight_decay)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params.lr_step_size, gamma=params.lr_gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21320fbf-2223-4dd6-93d9-ca04a336c144",
      "metadata": {
        "id": "21320fbf-2223-4dd6-93d9-ca04a336c144"
      },
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "checkpoint_path = os.path.join(\"checkpoints\", params.name, f\"checkpoint.pth\")\n",
        "if resume_training and os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint[\"model\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
        "    assert params == checkpoint[\"params\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0065791-1dea-462b-8e11-338f09132d98",
      "metadata": {
        "id": "b0065791-1dea-462b-8e11-338f09132d98"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from pathlib import Path\n",
        "Path(os.path.join(\"checkpoints\", params.name)).mkdir(parents=True, exist_ok=True)\n",
        "writer = SummaryWriter('runs/' + params.name)\n",
        "test(val_loader, model, loss_fn, epoch=0, writer=writer, train_dataloader=train_loader, calc_acc5=True)\n",
        "for epoch in range(start_epoch, 100):\n",
        "    train(train_loader, model, loss_fn, optimizer, epoch=epoch, writer=writer)\n",
        "    checkpoint = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"lr_scheduler\": lr_scheduler.state_dict(),\n",
        "        \"epoch\": epoch,\n",
        "        \"params\": params\n",
        "    }\n",
        "    torch.save(checkpoint, os.path.join(\"checkpoints\", params.name, f\"model_{epoch}.pth\"))\n",
        "    torch.save(checkpoint, os.path.join(\"checkpoints\", params.name, f\"checkpoint.pth\"))\n",
        "    lr_scheduler.step()\n",
        "    test(val_loader, model, loss_fn, epoch + 1, writer, train_dataloader=train_loader, calc_acc5=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WId5fLG556Tr",
      "metadata": {
        "id": "WId5fLG556Tr"
      },
      "outputs": [],
      "source": [
        "%mkdir /content/drive/MyDrive/projects/imagenet_poc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bXyyOFe7S1M",
      "metadata": {
        "id": "5bXyyOFe7S1M"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/checkpoints/ /content/drive/MyDrive/projects/imagenet_poc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JIE8joXp6MIr",
      "metadata": {
        "id": "JIE8joXp6MIr"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/projects/imagenet_poc/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X14LlLup6Og4",
      "metadata": {
        "id": "X14LlLup6Og4"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/runs /content/drive/MyDrive/projects/imagenet_poc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ue62OZtt6cSN",
      "metadata": {
        "id": "ue62OZtt6cSN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.13 (CUDA)",
      "language": "python",
      "name": "python313"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}